# LLM Cost & Performance Analyzer

This is a **Streamlit web app** that compares multiple Large Language Models (LLMs) like GPT-4, Claude, and Gemini.

It evaluates them on:
- **Speed** (response time)
- **Cost** (estimated token usage)
- **Output Length** (number of words)

## ğŸš€ How to Run Locally
1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Run the Streamlit app:
   ```bash
   streamlit run app.py
   ```

3. Open the link [http://localhost:8501](http://localhost:8501) in your browser.

## ğŸŒ How to Deploy
- Push this repo to GitHub.
- Deploy free on [Streamlit Cloud](https://streamlit.io/cloud).
- Share the live demo link with recruiters.

## ğŸ“Š Example Output
- Interactive table of model results
- Bar chart comparing duration, cost, and output length
- Sample responses for each model
