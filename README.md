# LLM Cost & Performance Analyzer

This is a Streamlit web app that benchmarks multiple Large Language Models (LLMs) â€” GPT-4, Claude, and Gemini.  
It evaluates them on:

- Speed (response time)  
- Cost (estimated token usage)  
- Output length (number of words)  

---

## Live Demo
ðŸ‘‰ [View the app here](https://llm-analyzer-app-9pmua4hyyn87cwzwkrzz73.streamlit.app/)  

---

## Features
- Interactive UI built with Streamlit  
- Model performance comparison with tables and charts  
- Dummy data for safe public demo (cost-free)  
- Easily extendable to real APIs (OpenAI, Claude, Gemini)  

---

## Tech Stack
- Python  
- Streamlit  
- Pandas  
- Matplotlib  

---

## Run Locally
Clone and run the app:
```bash
git clone https://github.com/vinny990/llm-analyzer-streamlit.git
cd llm-analyzer-streamlit
pip install -r requirements.txt
streamlit run app.py
```

Open in your browser: [http://localhost:8501](http://localhost:8501)

---

## Future Enhancements
- Connect to real LLM APIs (OpenAI, Claude, Gemini)  
- Add response quality scoring  
- Export PDF reports with results  

---

## Author
**Vinny Jindal**  
[LinkedIn](https://linkedin.com/in/vinny90) | [GitHub](https://github.com/vinny990)
