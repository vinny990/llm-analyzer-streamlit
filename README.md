# ğŸ¤– LLM Cost & Performance Analyzer

This is a **Streamlit web app** that benchmarks multiple Large Language Models (LLMs) â€” GPT-4, Claude, and Gemini.  
It evaluates them on:

- **Speed** â±ï¸ (response time)  
- **Cost** ğŸ’° (estimated token usage)  
- **Output Length** ğŸ“ (number of words)  

---

## ğŸ“Š Features
- Interactive UI built with **Streamlit**
- Side-by-side **model performance comparison**
- **Visual dashboards** with Pandas + Matplotlib
- Dummy data for public demo (safe, cost-free)
- Ready to integrate with real APIs (OpenAI, Claude, Gemini)

---

## ğŸ› ï¸ Tech Stack
- Python  
- Streamlit  
- Pandas  
- Matplotlib  

---

## ğŸš€ Run Locally
Clone and run the app:
```bash
git clone https://github.com/vinny990/llm-analyzer-streamlit.git
cd llm-analyzer-streamlit
pip install -r requirements.txt
streamlit run app.py
```

Open in your browser: [http://localhost:8501](http://localhost:8501)

---

## ğŸ”® Future Enhancements
- Connect to real LLM APIs (OpenAI, Claude, Gemini)  
- Add response quality scoring  
- Export PDF reports with results  

---

## âœ¨ Author
**Vineet "Vinny" Jindal**  
[LinkedIn](https://linkedin.com/in/vinny90) | [GitHub](https://github.com/vinny990)
