# LLM Cost & Performance Analyzer

This is a Streamlit web app that benchmarks multiple Large Language Models (LLMs) â€” GPT-4, Claude, and Gemini.  
It evaluates them on:

- Speed (response time)  
- Cost (estimated token usage)  
- Output Length (number of words)  

---

## Features
- Interactive UI built with Streamlit
- Side-by-side model performance comparison
- Visual dashboards with Pandas + Matplotlib
- Dummy data for public demo (safe, cost-free)
- Ready to integrate with real APIs (OpenAI, Claude, Gemini)

---

## Tech Stack
- Python  
- Streamlit  
- Pandas  
- Matplotlib  

---

## Run Locally
Clone and run the app:
```bash
git clone https://github.com/vinny990/llm-analyzer-streamlit.git
cd llm-analyzer-streamlit
pip install -r requirements.txt
streamlit run app.py
```

Open in your browser: [http://localhost:8501](http://localhost:8501)

---

## Future Enhancements
- Connect to real LLM APIs (OpenAI, Claude, Gemini)  
- Add response quality scoring  
- Export PDF reports with results  

---

## Author
**Vineet "Vinny" Jindal**  
[LinkedIn](https://linkedin.com/in/vinny90) | [GitHub](https://github.com/vinny990)
